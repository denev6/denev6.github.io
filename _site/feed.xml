

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://denev6.github.io/</id>
  <title>Palette</title>
  <subtitle>개발을 좋아하는 학부생의 노트입니다.</subtitle>
  <updated>2025-10-26T01:40:48+09:00</updated>
  <author>
    <name>박성진</name>
    <uri>https://denev6.github.io/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="https://denev6.github.io/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="ko-KR"
    href="https://denev6.github.io/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator>
  <rights> © 2025 박성진 </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>Less is more, Reasoning Model의 등장</title>
    <link href="https://denev6.github.io/study/2025/10/25/hrm.html" rel="alternate" type="text/html" title="Less is more, Reasoning Model의 등장" />
    <published>2025-10-25T00:00:00+09:00</published>
  
    <updated>2025-10-25T00:00:00+09:00</updated>
  
    <id>https://denev6.github.io/study/2025/10/25/hrm.html</id>
    <content src="https://denev6.github.io/study/2025/10/25/hrm.html" />
    <author>
      <name>박성진</name>
    </author>

  
    
    <category term="Study" />
    
  

  
    <summary>
      





      LLM을 활용한 연구가 쏟아지는 동시에 LLM의 한계도 점점 명확해져 가고 있다. 그로 인해 MicroSoft의 BitNet과 같이 작은 모델을 만들기 위한 시도가 계속 이어져왔다. 그러다 최근 몇 달 동안 reasoning 분야에서 새로운 아키텍쳐가 논의되고 있다. 오늘은 Hierarchical Reasoning Model이라고 불리는 아키텍쳐에 대해 이야기해 보려 한다.

인간의 뇌에서 영감을 받다


  Hierarchical Reasoning Model, 2025.


추론 문제를 풀기 위해 CoT(Chain-of-Thought) 같은 방법론이 제시되어 왔지만 기대만큼의 성능 향상을 보이지는 못했다. HRM(Hierarchical Reasoning Model)은 “latent reasoning”...
    </summary>
  

  </entry>

  
  <entry>
    <title>Agent에게 오답노트를 시켜봤더니</title>
    <link href="https://denev6.github.io/study/2025/10/19/agent-memory.html" rel="alternate" type="text/html" title="Agent에게 오답노트를 시켜봤더니" />
    <published>2025-10-19T00:00:00+09:00</published>
  
    <updated>2025-10-20T01:05:17+09:00</updated>
  
    <id>https://denev6.github.io/study/2025/10/19/agent-memory.html</id>
    <content src="https://denev6.github.io/study/2025/10/19/agent-memory.html" />
    <author>
      <name>박성진</name>
    </author>

  
    
    <category term="Study" />
    
  

  
    <summary>
      





      

Fine-tuning은 가성비가 떨어진다


  A Survey on In-context Learning, 2024.


LLM은 최근 많은 분야에서 활용되고 있다. 특정 과제에서 LLM의 성능을 높이기 위해 지도학습 + 강화학습 개념의 fine-tuning 기법이 사용되고 있다. 하지만 fine-tuning은 가성비가 떨어진다. 우선, 계산 비용이 아주 비싸다. 물론 LoRA를 필두로 효율적인 학습 방법이 제안되고 있지만, LLM을 돌린다는 것만으로 엄청난 GPU 연산이 들어간다. 이렇게 학습된 모델은 특정 downstream task만 수행할 수 있다. 특정 작업은 잘 하게 되었지만, LLM이 가지는 범용성은 포기해야 한다. 뿐만 아니라 fine-tuning을 잘 하기 위해서는 많은 양질의 데이...
    </summary>
  

  </entry>

  
  <entry>
    <title>RL Agent와 인간은 어떻게 협업해야 할까</title>
    <link href="https://denev6.github.io/study/2025/10/11/agent-human-comm.html" rel="alternate" type="text/html" title="RL Agent와 인간은 어떻게 협업해야 할까" />
    <published>2025-10-11T00:00:00+09:00</published>
  
    <updated>2025-10-20T00:52:16+09:00</updated>
  
    <id>https://denev6.github.io/study/2025/10/11/agent-human-comm.html</id>
    <content src="https://denev6.github.io/study/2025/10/11/agent-human-comm.html" />
    <author>
      <name>박성진</name>
    </author>

  
    
    <category term="Study" />
    
  

  
    <summary>
      





      최근 능동적으로 생각하고, 계획하고, 행동하는 Agent가 다양한 분야에서 연구되고 있다. 그리고 Agent 간의 협업을 통해 과제를 수행하는 Multi-Agent가 주목받고 있다. 관련 연구를 따라가다 보니 재밌는 직관이 생각나 글을 통해 풀어보려고 한다.

Agent 간 협업 이해하기


  Improving Factuality and Reasoning in Language Models through Multiagent Debate, 2024.

  Learning to Ground Multi-Agent Communication with Autoencoders, 2021.


Agent 간의 협업을 ‘인간이 해석할 수 있는가’에 따라 2가지로 나눌 수 있다.


  자연어를 이용한 LLM 간의 소통...
    </summary>
  

  </entry>

  
  <entry>
    <title>Co-learning을 활용한 멀티모달 학습</title>
    <link href="https://denev6.github.io/study/2025/09/20/co-learning.html" rel="alternate" type="text/html" title="Co-learning을 활용한 멀티모달 학습" />
    <published>2025-09-20T00:00:00+09:00</published>
  
    <updated>2025-10-06T02:15:43+09:00</updated>
  
    <id>https://denev6.github.io/study/2025/09/20/co-learning.html</id>
    <content src="https://denev6.github.io/study/2025/09/20/co-learning.html" />
    <author>
      <name>박성진</name>
    </author>

  
    
    <category term="Study" />
    
  

  
    <summary>
      





      ViLBERT, SimCLR, CLIP, ImageBind 4가지 모델을 예시로, 이미지와 다른 모달리티 간 co-learning을 살펴본다.

Co-learning

Multi-modal learning에서 co-learning은 모달리티를 학습하기 위해 다른 모달리티를 활용하는 방법을 말한다. 예를 들어, “강아지”라는 텍스트의 특성을 파악하기 위해 강아지 이미지를 함께 학습하는 식이다.

본 글은 대표적인 두 모델 ViLBERT와 CLIP을 살펴본다.

ViLBERT


  ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks




기본적으로 BERT 구조를 사용해 특징을 추출...
    </summary>
  

  </entry>

  
  <entry>
    <title>PyCon KR 2025</title>
    <link href="https://denev6.github.io/experience/2025/08/17/pycon2025.html" rel="alternate" type="text/html" title="PyCon KR 2025" />
    <published>2025-08-17T00:00:00+09:00</published>
  
    <updated>2025-08-17T00:00:00+09:00</updated>
  
    <id>https://denev6.github.io/experience/2025/08/17/pycon2025.html</id>
    <content src="https://denev6.github.io/experience/2025/08/17/pycon2025.html" />
    <author>
      <name>박성진</name>
    </author>

  
    
    <category term="Experience" />
    
  

  
    <summary>
      





      

PyCon 2025에 다녀왔다. 처음 참여하는 개발 컨퍼런스라 긴장 반 설렘 반으로 참여했다. 컨퍼런스라고 하면 전문가분들이 모이는 진지한 자리 같아서 멀게만 느껴졌지만, 현업에서 Python을 사용하시는 분들의 이야기가 듣고 싶어 참여를 결정했다.

Django와 HTMX에 설득되다



필자는 주로 ML 모델을 다루다보니 벡엔드는 API를 던지고 받는 정도만 해왔다. 그렇다보니 고민 없이 FastAPI만 사용해왔고, Django는 머리 속에서 잊혀져 있었다. 그러다 여러 세션에서 Django의 장점을 듣게 되었다. 특히 세션 로그인이나 DB migration 등 작업을 Django가 아주 쉽게 할 수 있다는 내용이었다. 물론 앞으로도 간단한 API 서버는 FastAPI로 열겠지만, 머리 속에 D...
    </summary>
  

  </entry>

</feed>


