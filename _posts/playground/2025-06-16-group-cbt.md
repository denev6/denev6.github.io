---
title: 그룹 채팅 환경에서의 LLM 개입
tags: [AI, LLM]
category: Playground 
toc: true
img_path: /assets/posts/group-cbt/
---

## 그룹 대화에 LLM이 개입하기 어려운 이유

> Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations, 2024.

1:1 대화는 주어진 질문에 적절한 답변을 생성하면 된다. 하지만 그룹 대화에서는 더 많은 요소를 고려해야 한다.

- What: 어떤 대답을 할지
- When: 지금 대답할지 말지
- Who: 누구에게 대답할지

이러한 종합적인 판단을 내리기 위해 Microsoft는 MUCA: Multi-User Chat Assistant를 제안했다.

![MUCA framework](muca.png)

MUCA 크게 Sub-topics Generator, Dialog Analyzer, Conversational Strategies Arbitrator로 구성되어 있다. Sub-topics Generator는 T개의 세부 대화 주제를 초기화한다. Dialog Analyzer는 유용한 정보를 뽑아내는 모듈로, 세부 주제 업데이트 / 대화 특징 추출 / 대화 요약 업데이트 / 참가자 특징 추출을 수행한다. 다음으로 Conversational Strategies Arbitrator가 정보를 전달받아 최종 행동을 결정한다. 행동은 직접 대화 / 요약 생성 / 참가 독려 / 세부 주제 전환 / 갈등 해결 / 대화 개입 / 침묵하기 중에 선택할 수 있다.

## 개입할 타이밍을 찾는 법

> HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance, 2024.

그렇다면 구체적으로 개입할 타이밍은 어떻게 정해야할까? HuixiangDou는 Text2Vec과 LLM scoring을 제안한다. 본 시스템은 기술적인 대화를 나누는 그룹 채팅에 사용된다. 따라서 미리 기술에 관한 DB를 구축해 두었다. 사용자 입력이 들어오면 먼저 DB에서 입력과 저장된 문장 간의 거리를 비교한다. 이를 통해 너무 멀리 있으면 무시해도 되는 대화로 판단한다. 하지만 이 방식은 단어 선택에 매우 민감하게 반응한다. 예를 들어, *"This development board is very good"*와 *"This board is poorly designed"*는 높은 유사도를 가진다. 이 문제를 해결하기 위해 추가로 LLM scoring을 거친다. LLM이 해당 입력이 대답할 가치가 있는지 0-10 척도로 평가한다.

![Huixiangdou pipeline](huixiangdou.png)

## 간단한 실험

위 지식을 바탕으로 그룹 대화 상황에서 사용할 수 있는 간단한 챗봇을 만들어 보았다. CBT는 Cognitive Behavior Therapy로 주목받는 심리 상담 기법이다. 당시('25.04) 연구실에서 심리 치료 챗봇이 연구되고 있었기에 CBT 치료를 주제로 그룹 채팅 에이전트를 기획했고, LangGraph framework을 이용해 DAG(Dynamic Allocation Graph)를 구현했다.

![framework](flow.png)

### 개입 결정

에이전트는 CBT 지식을 바탕으로 대화에 자율적으로 개입할지 결정한다. 이전 연구를 따라, 사용자 대화 중 CBT 개입이 필요한 질문이나 발화에 대해서만 개입한다. 일상적인 대화 주제나 단순한 공감 표현으로 판단되면 행동을 취하지 않는다. 에이전트는 개입이 필요한 경우 1을, 개입이 필요하지 않은 경우 0을 반환한다. 1을 반환할 때 에이전트는 발화를 바탕으로 다음 행동을 선택하며, 0을 반환할 때 판단 과정을 종료한다.

### 행동 선택

개입이 필요한 경우, 에이전트는 미리 정의된 4가지 행동 중 가장 적절한 행동을 결정한다. 에이전트 행동은 다음과 같이 정의된다: (1) 대화 요약, (2) 핵심 사고와 신념 식별, (3) 전략 또는 통찰 제공, (4) 참여자 격려. 이러한 선택지 중 적절한 것이 없다고 판단되면 에이전트는 개입 없이 종료할 수 있다.

**대화 요약**. 대화 요약은 그룹 채팅에서 사용자들이 대화 맥락을 이해하는 데 도움을 주며, 높은 만족도를 보이는 행동으로 평가된다. History Manager로부터 대화 맥락을 받아 사용자들에게 제공한다.

**핵심 사고와 신념 식별**. CBT에서 자동적 사고와 핵심 신념을 식별하는 것은 내담자의 정서적 반응과 문제 행동 뒤에 있는 인지적 기제를 이해하기 위한 중요한 절차다. 핵심 신념 식별 과정은 비합리적 사고를 인식하고 인지 재구성의 기초를 확립하는 데 도움을 준다. 이는 치료 효과성과 지속성을 높일 수 있다.

**전략 또는 통찰 제공**. 시스템은 사용자의 변화 준비도에 따라 격려의 정도를 조절하여 맞춤형 조언을 제공한다. 범이론적 행동변화 모델(TTM, Transtheoretical model)은 행동 변화 단계에 따라 내담자를 5개 그룹으로 분류하는 모델이다. 각 단계의 내담자에게 다른 상담 전략을 적용하여 치료 성공 확률을 높인다. 따라서 행동 변화 의지가 있는 Preparation/Action 단계 사용자에게는 적극적인 행동 변화를 제안한다. 반대로 Contemplation 단계 사용자는 구체적 제안보다는 자기결정을 통해 자율적으로 변화 의지를 기를 수 있는 격려의 말을 받는다.

**참여자 격려**. 사용자의 변화나 성취를 격려하여 자기효능감을 강화한다. 단, 구체적인 조언 제공 없이 격려에만 집중한다.

### 시나리오 평가

그룹 대화 상황에서 발생할 수 있는 시나리오를 생성하고 챗봇이 적절한 응답을 생성하는지 검증했다. 답변 생성 모델로는 Claude-3-7-sonnet을, router로는 GPT-4o-mini를 사용했다 (각 모델의 무료 토큰을 제공받았기 때문이다). 전체 프레임워크는 Python 3.10으로 구현되었다. 온라인 채팅 환경을 가정하여 구어체 표현과 텍스트 기호를 함께 사용했다.

다음은 테스트에 사용된 입력과 챗봇🤖의 응답이다.

구체적인 결과: [Github](https://github.com/denev6/cbt-group-chat/blob/main/test.md)

> 🐕: 안녕하세요 :) 잘 지내셨나요?
>
> 🐨: 오늘 친구한테 인사했는데 무시했어요... 절 싫어하나봐요
>
> 🤖: 친구가 당신을 무시한 것처럼 느껴져서 속상하셨겠네요. 하지만 친구가 당신을 싫어해서가 아니라 다른 이유(피곤함, 급한 일, 생각에 빠져있음 등)가 있을 수도 있어요.
>
> 🐕: 속상했겠네요..ㅠㅠ
>
> 🐰: 저도 그런 적이 있었는데 제가 오해한 거 더라고요..ㅎㅎ
>
> 🐨: 조금 더 긍정적으로 생각해 봐야겠네요. 도움이 될만한 방법이 있을까요?
>
> 🤖: 정말 좋은 마음가짐이에요! 다음에 친구를 만나면 가볍게 "지난번에 바빴니?" 하고 물어보는 것도 좋을 것 같아요. 오해가 풀릴 기회가 생기고, 상대방의 상황을 이해할 수 있게 될 거예요. 긍정적인 태도가 관계 개선에 큰 도움이 됩니다.
>
> 🐢: 제가 방금 봐서 그런데... 앞에 어떤 대화를 하셨었죠?
>
> 🤖: 이때까지 나눴던 대화를 요약해 볼게요. 참여자가 친구의 인사 무시로 인한 부정적 감정을 표현했고, 다른 참여자가 자신의 유사 경험을 공유하며 긍정적 재해석을 제안했습니다. 인지적 왜곡(친구가 나를 싫어한다는 자동적 사고)을 인식하고 대안적 설명(피곤함, 급한 일 등)을 고려하는 인지 재구성 과정이 이루어졌으며, 직접 대화를 통한 오해 해소 방법이 제안되었습니다.

"오늘 친구한테 인사했는데 무시하더라구요... 저를 싫어하는 것 같아요"라는 문장은 다른 사람의 생각을 추측하는 것으로, 전형적인 인지 왜곡인 'All-or-Nothing Thinking'를 나타낸다. 챗봇은 '핵심 사고와 신념 식별'을 통해 사용자가 사실을 인지적으로 왜곡할 수 있음을 식별했다. 이후 "속상하셨겠어요", "저도 그런 경험이 있었어요"와 같은 공감 표현에는 응답하지 않고 넘어갔다. "도움이 되는 방법이 있을까요?"라는 질문에 대해서는 이전 대화 맥락을 이해하고 사용자에게 적절한 조언을 제안했다. 마지막으로 "앞서 무슨 대화를 하고 계셨나요?"와 같이 대화 맥락을 놓친 사용자에게는 대화 참여를 도울 수 있도록 요약을 제공했다.

## 문제점

간단한 시나리오를 통해 의도한대로 답변을 잘 생성하는 것을 볼 수 있다. 하지만 이 방식은 매우 비효율적이다. 모든 입력을 LLM이 판단해야하기 때문에 낭비되는 토큰 비용이 매우 크다. 만약 사용자가 "아니 근데" - "ㅋㅋㅋㅋㅋㅋ" - "말이 안 되잖아"와 같이 문장을 끊어서 입력한다면 더 많은 LLM 호출이 발생한다. 이러한 문제에 대한 해답을 오래된 논문에서 찾을 수 있었다.

## 인간의 도움을 받아보자

> Making Sense of Group Chat through Collaborative Tagging and Summarization, 2018.

이 논문이 공개될 당시는 2018로 LLM이라는 개념이 없을 때였다. 그래서 인간이 직접 중요한 문장에 태그를 다는 방식을 제안했다.

![Notes and Tags](tags.png)

물론 '업무 메신저'라는 특수한 상황이었기 때문에 중요한 내용을 인간이 직접 정리하는 방식이 자연스럽게 들리는 것도 있다. 하지만 대화 에이전트를 인간과 동등한 수준으로 보지 않고, 대화 보조 도구 정도로 바라본다면 도움을 받기 위해 인간이 직접 표시를 남기는 방식도 충분히 합리적이라고 생각한다. 좋게 포장하면 Human-in-the-loop을 적용한다고 표현할 수 있다.

---

**Reference**

- Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations, 2024.
- HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance, 2024.
- Making Sense of Group Chat through Collaborative Tagging and Summarization, 2018.
