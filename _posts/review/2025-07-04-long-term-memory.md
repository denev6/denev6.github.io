---
title: 네이버가 말하는 의료 챗봇과 장기 기억
tags: [NLP, HCI]
category: Review
toc: true
---

이 글에서는 **'신뢰 가능한' 의료 AI 챗봇**에 대한 고민을 다루고 있어요. 네이버(NAVER)의 연구를 중심으로 설명하며, '장기기억(Long-term memory)'이라는 중요한 키워드에 대해 이야기해요.

## 내담자는 상담자에게 솔직한가?

심리 상담 치료에서는 **내담자와 상담자의 신뢰 관계 형성이 중요**해요. 내담자가 상담자를 신뢰하지 않으면 진솔한 이야기를 털어놓기 어려워요. 내담자가 정보를 제공하지 않으면, 정확한 진단과 치료가 힘들어지죠. 이론적으로 완벽한 치료 세션이 있더라도 내담자의 정보가 없으면 소용이 없어요. 그렇다면 의료 챗봇의 경우는 어떨까요?

최근 LLM을 활용한 의료 챗봇이 활발히 연구되고 있어요. 기존에 인간 전문가가 수행하던 세션을 챗봇이 대신하는 경우가 많아요. 그러나 이론적 배경이 완벽하더라도 신뢰 관계가 형성되지 않으면 치료에 진전이 더딜 수 있어요. **인간은 인공지능 챗봇을 신뢰하는가?** 이에 대한 답은 '하기 나름'이에요.

## 믿을 수 있는 챗봇을 만들자

네이버는 [CareCall](https://www.ncloud.com/product/aiService/clovaCareCall)이라는 서비스를 운영하고 있어요. CareCall은 '돌봄이 필요한 대상자에게 AI가 주기적으로 전화를 걸어 건강, 식사, 수면 등과 관련된 일상적인 안부를 묻고 이상 징후를 모니터링하는 AI 안부 확인 서비스'예요. CareCall의 주요 특징 중 하나는 '장기 기억'이에요. 네이버는 2024년 [연구 논문](https://dl.acm.org/doi/10.1145/3613904.3642420)에서 LLMs with long-term memory(LTM)의 효과를 설명했어요.

LTM은 이전 세션에서 나눴던 대화를 기억하고, 다음 세션에도 이어서 질문하는 기술을 말해요. 이 기술은 계속 변하는 사용자의 건강 상태를 업데이트하며 개인 맞춤형 의료 서비스를 제공해요. 이 연구는 LTM의 효과를 입증하기 위해 1,252개의 대화 로그를 양적/질적으로 분석했어요. 이를 통해 밝혀낸 효과는 다음과 같아요.

- 시간이 지남에 따라 사용자는 자신의 건강 상태를 솔직하게 밝히게 돼요.
- 챗봇을 향한 부정적인 시각이 줄어들어요.

### 자세한 건강 정보를 이끌어내다

LTM을 경험한 사용자는 자신의 건강에 대해 솔직하게 이야기하기 시작했어요. 예를 들어, 한 사용자는 불면증에 대해 챗봇과 이야기를 나누었고, 챗봇이 이를 기억하고 다시 질문했을 때 더 자세한 정보를 제공했어요. 다른 사용자는 AI와 대화하는 게 처음에는 어색했지만, 점차 더 자세한 이야기를 하게 되었어요. 또 다른 사용자는 처음에는 자세한 이야기를 하지 않다가 점점 세세한 정보를 공유하기 시작했어요.

> "나이가 들어서 여기저기가 다 아파. 설명하기 쉽지 않네"
> → "오랫동안 불면증이 있었어, 15년 정도"
> → "정신과에 다니곤 했는데, 약에 내성이 생겨서 이제는 더 이상 효과가 없어"

이처럼 LTM을 경험한 사용자는 시간이 지남에 따라 본인의 건강 정보를 솔직하게 드러내는 경향이 있었어요. 반면 **LTM을 경험하지 않는 그룹은 비슷한 대답을 반복하는 경향**을 보였어요.

> 챗봇: 어디 아픈 곳 있나요?
>
> 사용자: "허리 수술하고 나서, 다리 쪽에 만성적인 통증이 있어"
>
>
> 챗봇: 어디 불편한 곳 있나요?
>
> 사용자: "허리랑 다리가 많이 아파"

이 경우처럼 챗봇의 일반적인 질문에 대해 자세히 답하지 않았어요.

### 친근하게 다가가다

LTM을 도입한 그룹은 챗봇에게 *감사하다*고 말하며 친근감을 표현하는 경향이 있었어요. 여러 사용자가 직접적으로 *"고마워"*라고 표현했으며, 인터뷰에서도 긍정적인 반응을 보였어요. 한 사용자는 초반에 부정적인 태도를 보였지만, 시간이 지나면서 솔직하게 이야기하기 시작했어요. 또 다른 사용자는 처음에는 로봇과 대화하기 싫다고 했지만, 점차 긍정적인 태도로 변화했어요.

반대로 **LTM을 경험하지 않은 그룹은 정보를 기억하지 못할 때 화를 내기도 했어요.** 한 사용자는 반복되는 질문에 지친 모습을 보이며 부정적인 반응을 보였어요.

### 장기 기억이 항상 정답은 아니다

**만성 질환을 가진 환자에게는 LTM이 효과가 없었어요.** 만성 질환을 앓는 환자에게 LTM은 단순하고 반복된 대답만 들었어요.

> 챗봇: 허리 통증은 어때요?
>
> 사용자: "맨날 똑같지, 맨날 똑같아"
>
> 챗봇: 의사를 만나고 있나요?
>
> 사용자: "응"
>
> 챗봇: 꾸준히 치료를 받고 있으니까 분명 좋아질 거에요.
>
> 사용자: "아닐 걸. 난 너무 나이가 많아, 86이야"
>
>
> 챗봇: 허리 통증은 어때요?
>
> 사용자: "이건 뭐 달라지는 게 아니야"
>
> 챗봇: 의사를 다시 만났나요?
>
> 사용자: "그냥 척추가 닳았대"

LTM을 경험하며 개인정보 문제를 걱정하는 사용자도 있었어요. 너무 세세한 정보까지 기록되는 것에 불편함을 느낀 사용자도 있었죠.

장기 기억을 기반으로 질문을 생성할 때 '만성 질환 환자'를 고려하며, 너무 자세한 내용을 묻지 않도록 섬세하게 디자인해야 해요.

## 장기기억은 어떻게 만들까

장기 기억을 구현하는 방법은 2022년 [논문](https://arxiv.org/abs/2210.08750)에서 밝혀졌어요. 핵심은 정보를 계속 업데이트하며 최신 정보가 잘 반영될 수 있도록 하는 거예요. 이를 위해 세션이 끝날 때마다 대화를 요약하고, 이전에 저장된 기억과 비교해 { PASS, REPLACE, APPEND, DELETE } 연산을 수행해요.

- **PASS**: 기존 정보와 새로운 요약이 겹칠 때, 기존 정보만 유지해요.
- **REPLACE**: 기존 정보와 새로운 요약이 다를 때, 최신 정보로 대체해요.
- **APPEND**: 기존에 없는 새로운 요약이 생길 때, 새로운 정보를 추가해요.
- **DELETE**: 과거 정보는 잊고 새로운 정보는 저장하지 않아요.

DELETE가 필요한 예시는 다음과 같아요.

- 기존 정보: "감기가 있어 약을 복용하고 있음"
- 새로운 정보: "감기가 다 나았음"

위 예시에서 '감기가 있다'는 정보는 오래된 정보로 더 이상 필요하지 않아 삭제해요. 동시에 감기가 나았다는 정보도 저장할 이유가 없죠. 이러한 정보가 사라지지 않으면 'Pink Elephant Paradox'를 일으켜 hallucination을 초래할 수 있어요.

> Pink Elephant Paradox는 어떤 생각을 억누를수록 더 강하게 떠오르는 현상을 말해요. "지금부터 핑크 코끼리를 생각하지마"라고 하면 머리 속에 코끼리가 떠오르는 식이에요. 이렇게 불필요한 정보를 저장하고 있으면 시스템이 불필요한 생각을 하게 돼요.
{: .prompt-info }

연구는 T5 모델을 fine-tune해 분류 모델을 학습시켰고, 88%가 넘는 정확도로 올바른 연산을 잘 선택했어요.

참고로, 위 연구는 2022년에 제안된 기법이고 최근(2025년)에는 [Mem0](https://arxiv.org/html/2504.19413v1)라는 기법이 제안되어 오픈소스로 공개되었어요. Mem0는 의료 인공지능보다 개방형 대화 요약을 목적으로 설계되었고, 메시지를 비동기로 요약하는 extraction phase와 { ADD, UPDATE, DELETE, NOOP } 연산을 수행하는 update phase를 가지고 있어요. 네이버는 분리된 분류 모델(T5)을 학습해 연산을 지정했지만, Mem0는 GPT-4o-mini의 reasoning을 활용했어요.

## 결론

장기 기억(LTM)을 활용하면 내담자(사용자)의 자세한 건강 정보를 이끌어 낼 수 있어요. 또한 사용자가 챗봇을 더 친근하게 느끼고 긍정적인 시각으로 바라볼 수 있게 돼요. 이를 구현하기 위해 대화 요약과 정보 연산을 통해 최신 정보가 유지될 수 있도록 하는 방법이 제시되었어요. 하지만 장기 기억이 항상 긍정적인 결과로 이어지지는 않아요. 장기 기억을 이용해 대화를 이어갈 때 사용자가 불편하지 않도록 정교하게 질문을 생성할 필요가 있어요.

## 참고자료

- Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention: [ACM](https://dl.acm.org/doi/10.1145/3613904.3642420)
- 클로바 케어콜 논문(2): 챗봇의 장기기억과 자기표현: [CLOVA TECH-BLOG](https://clova.ai/tech-blog/ko-clova-%EC%BC%80%EC%96%B4%EC%BD%9C-%EC%97%B0%EA%B5%AC-%EB%85%BC%EB%AC%B8-%EC%86%8C%EA%B0%9C-2)
- Keep Me Updated! Memory Management in Long-term Conversations: [arXiv](https://arxiv.org/abs/2210.08750)
- Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory: [arXiv](https://arxiv.org/html/2504.19413v1)
- Advancing Conversational Psychotherapy: Integrating Privacy, Dual-Memory, and Domain Expertise with Large Language Models: [arXiv](https://arxiv.org/abs/2412.02987)
- MemoryBank: Enhancing Large Language Models with Long-Term Memory: [arXiv](https://arxiv.org/abs/2305.10250)
- In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents: [arXiv](https://arxiv.org/abs/2503.08026)
